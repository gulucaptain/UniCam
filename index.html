<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="UniCam: Taming Unified Diffusion Models in Noise Space for Camera-controllable Video Rendering">
  <meta property="og:title" content="UniCam"/>
  <meta property="og:description" content="UniCam: Taming Unified Diffusion Models in Noise Space for Camera-controllable Video Rendering"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="assets/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CameraNoise</title>
  <link href="static/css/style.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/x-icon" href="assets/images/CameraNoise.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<style>
  td {
    padding: 5px;
  }
</style>

<style>
  /* 导航整体样式 */
  #toc {
    position: fixed;          /* 固定在页面左上角 */
    top: 20px;
    left: 20px;
    background: #ffffff;      /* 白色背景 */
    border: 1px solid #ddd;   /* 边框 */
    border-radius: 8px;       /* 圆角 */
    padding: 15px 20px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1); /* 阴影 */
    font-family: "Arial", sans-serif;
    max-width: 200px;
  }
  
  /* 列表样式 */
  #toc ul {
    list-style: none;  /* 去掉默认圆点 */
    padding: 0;
    margin: 0;
  }
  
  /* 列表项样式 */
  #toc li {
    margin-bottom: 10px;
  }
  
  /* 链接样式 */
  #toc a {
    text-decoration: none;
    color: #333;
    font-weight: 500;
    transition: all 0.3s;
  }
  
  /* 悬停效果 */
  #toc a:hover {
    color: #007bff;
    transform: translateX(4px);
  }
  
  /* 当前高亮链接（可配合 JS 动态添加类 active） */
  #toc a.active {
    color: #007bff;
    font-weight: 700;
  }
  </style>

<body>
  <section class="section hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <span style="background: linear-gradient(to right, red, hotpink, violet); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">UniCam</span>: Taming Unified Diffusion Models in Noise Space for Camera-controllable Video Rendering
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Haoyu Zhao; Zuxuan Wu; Yu-Gang Jiang</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Fudan University</span>
            </div>
            <p>* This project page contains a large number of videos, please wait patiently for them to load.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<nav id="toc">
  <ul>
    <li><a href="#"><b>Table of Contents</b></a></li>
    <li><a href="#section1">1. I2V Generation</a></li>
    <li><a href="#section2">2. CameraNoise</a></li>
    <li><a href="#section3">3. More I2V Results</a></li>
    <li><a href="#section4">4. T2V Generation</a></li>
    <li><a href="#section5">5. More T2V Results</a></li>
    <li><a href="#section6">6. Framework</a></li>
    <li><a href="#section7">7. GRFlow</a></li>
    <li><a href="#section8">8. Optical flow</a></li>
    <li><a href="#More_results_1">9. Dynamic scenes</a></li>
    <li><a href="#More_results_2">10. Cross cameras</a></li>
    <li><a href="#More_results_3">11. Driving scenes</a></li>
    <li><a href="#More_results_4">12. Camera transfer</a></li>
    <li><a href="#More_results_5">13. Comparisons 1</a></li>
    <li><a href="#More_results_7">14. OOD scenarios 1</a></li>
  </ul>
</nav>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Controlling camera pose in video diffusion models is essential for novel-view video rendering, yet existing approaches struggle to achieve precise control. Methods that directly inject numerical camera parameters into the diffusion backbone often fail to capture subtle viewpoint variations and lead to structural distortions or visual artifacts. To overcome these limitations, we propose UniCam, a unified framework that introduces a temporally coherent stochastic representation, termed CameraNoise, warped from camera intrinsic and extrinsic parameters. Unlike conventional approaches, CameraNoise embeds camera poses directly into the noise space. This makes our approach independent of scene appearance while faithfully encoding camera motion. Specifically, we introduce a novel Geometry-guided Reprojection Flow along with a CameraNoise warping algorithm, which jointly preserves the Gaussian prior of diffusion and ensures consistent noise propagation under camera transformations. By integrating CameraNoise into the diffusion process, the UniCam framework produces stable, high-quality videos with precise camera control across text-to-video, image-to-video, and video-to-video generation tasks. Extensive experiments on three public benchmarks demonstrate that our approach significantly outperforms prior methods in both fidelity and controllability.
          </p>
          <!-- <img style="max-width: 400px; display: block; margin: 0 auto;" src="assets/figs/ots.png" /> -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero" id="section1">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>1. Camera-controllable Image-to-Video (I2V) Generation</h2>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> Our method transforms camera poses into CameraNoise, a noise-space representation of camera motion. This appearance-independent representation is introduced into the denoising process of the video diffusion model, enabling the generated videos to accurately capture camera motion trajectories.
      </div>
      <table class="video-table" >
        <tr>
          <td><video src="assets/generated_process/process_demo1.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/generated_process/process_demo2.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/generated_process/process_demo3.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/generated_process/process_demo4.mp4" loop controls muted autoplay></video></td>
        </tr>

      </table>
    </div>
  </div>
</section>

<!-- ---------------- BEGIN Showcases SECTION ------------------>
<!-- Video carousel -->
<section class="section hero is-light" id="section2">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>2. Warped CameraNoise from Camera Poses</h2>
      <h2 class="title is-5"><span class="todo">[TODO #2]</span>The proposed CameraNoise is an appearance-agnostic noise representation.</h2>
      
      <table style="max-width: 1500px; margin: auto; width: 100%; table-layout: fixed;">
        <tr>
          <td><video src="assets/cameranoise/demo1.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/cameranoise/demo2.mp4" loop controls muted autoplay></video></td>
        </tr>
        
        <tr>
          <td><video src="assets/cameranoise/demo3.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/cameranoise/demo4.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/cameranoise/demo5.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/cameranoise/demo6.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/cameranoise/demo7.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/cameranoise/demo8.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> You can pause the video at any frame to examine the Gaussian distribution of a single frame. At that moment, you can notice that the motion information in the video has disappeared.
      </div>
    </div>
  </div>
</section>


<!-- Video carousel2 -->
<section class="section hero" id="section3">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>3. More Camera Controllable I2V Results</h2>
      
      <table class="video-table">
        <!-- Row 1 -->
        <tr>
          <td><video src="assets/i2v_results/8f04f919b046336c_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/i2v_results/56ae4fff81255579_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/i2v_results/93a8bf0ecd7eafcf_RIGHT.mp4" loop controls muted autoplay></video></td>
        </tr>

        <!-- Row 2 -->
        <tr>
          <td><video src="assets/i2v_results/deb368fb90770550_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/i2v_results/94e35563d865a2d6_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/i2v_results/7526c18f191bcc1a_RIGHT.mp4" loop controls muted autoplay></video></td>
        </tr>

        <!-- Row 3 -->
        <tr>
          <td><video src="assets/i2v_results/bf756257ffdd0017_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/i2v_results/c45cab04c3b22166_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/i2v_results/dd1c1d26525a2a1b_RIGHT.mp4" loop controls muted autoplay></video></td>
        </tr>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> Each video is generated with reference image, camera poses, and prompt.
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light" id="section4">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #4]</span>4. Camera Controllable Text-to-Video (T2V) Generation</h2>

      <table class="video-table">
        <!-- Row 1 -->
        <tr>
          <td><video src="assets/t2v_process/t2v_process1.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/t2v_process/t2v_process2.mp4" loop controls muted autoplay></video></td>
        </tr>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #5]</span> Each Videos is generated with a textual prompt and camera poses.
      </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero" id="section5">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #4]</span>5. More Camera Controllable T2V Results</h2>

      <table class="video-table">
        <!-- Row 1 -->
        <tr>
          <td><video src="assets/t2v_results/1758368084_Image_c3c33ceed1308b42_Camera_c3c33ceed1308b42_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/t2v_results/1758359463_Image_dd1c1d26525a2a1b_Camera_dd1c1d26525a2a1b_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/t2v_results/1758359720_Image_deb368fb90770550_Camera_deb368fb90770550_RIGHT.mp4" loop controls muted autoplay></video></td>
        </tr>

        <!-- Row 3 -->
        <tr>
          <td><video src="assets/t2v_results/1758362852_Image_1b2937e192040745_Camera_1b2937e192040745_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/t2v_results/1758363480_Image_dd5288bacc7da7cf_Camera_dd5288bacc7da7cf_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/t2v_results/1758363694_Image_2f7f2369486cc959_Camera_2f7f2369486cc959_RIGHT.mp4" loop controls muted autoplay></video></td>
        </tr>

        <!-- Row 4 -->
        <tr>
          <td><video src="assets/t2v_results/1758364115_Image_5151d3969e328df1_Camera_5151d3969e328df1_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/t2v_results/1758365812_Image_645cc7949386d427_Camera_645cc7949386d427_RIGHT.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/t2v_results/1758366227_Image_a552d52c34c2c920_Camera_a552d52c34c2c920_RIGHT.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel2 -->

<section class="section hero is-light" id="section6">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>6. Framework</h2>
      <img style="max-width: 1000px; margin: 0 auto;" class="fit-picture" src="assets/framework/framework.png" alt="Grapefruit slice atop a pile of other slices" />
      
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> Overview of our framework. We introduce CameraNoise, a controlled noise signal that encodes temporal correlations of camera poses into video diffusion. Our method is constructed via Geometry-guided Reprojection Flow (GRFlow) and a Gaussian-preserving warping algorithm, and injected into the video diffusion to enable precise viewpoint control. We use bold green arrows to illustrate the flow of control signals from camera poses to the synthesized video.
      </div>
    </div>
  </div>
</section>

<section class="section hero" id="section7">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>7. Geometry-guided Reprojection Flow (GRFlow) </h2>
      <h2 class="title is-4"><span class="todo">[TODO #2]</span>A Reprojection of Camera Poses in 2D Grid.</h2>
      
      <table class="video-table">
        <tr>
          <td><video src="assets/gr_flow/grflow_demo1.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> In this example, we show the GRFlows generated for the leftmost video under different alpha values in Eq. (5).
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light" id="section8">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>8. Our Appearance-agnostic CameraNoise via GRFlow</h2>
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>v.s.</h2>
      <h2 class="title is-4"><span class="todo">[TODO #2]</span>Appearance-motion entangled optical-flow-based noise</h2>
      
      <table class="video-table">
        <tr>
          <td><video src="assets/optical_vs_grflow/comparison_demo1.mp4" loop controls muted autoplay></video></td>
        </tr>
        
        <tr>
          <td><video src="assets/optical_vs_grflow/comparison_demo2.mp4" loop controls muted autoplay></video></td>
        </tr>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> * <i>To enhance the visualization of motion and appearances in the noise, we speed up the video x2.</i>
      </div>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> Since optical flow inherently contains object contours and appearance information, the warped noise derived from it inevitably carries appearance priors. During inference in diffusion models, such information can conflict semantically with the noise prior and control conditions, ultimately leading to generation failure.
      </div>
    </div>
  </div>
</section>


<!-- Video carousel2 -->
<section class="section hero is-light" id="More_results_1">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>9. Dynamic and ourdoor scenes with different camera poses.</h2>
      
      <p>Camera1: Move-Up Shot.</p>
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case1.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case2.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case3.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case4.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>

      <p>Camera2: Counterclockwise Rotation Shot.</p>
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case5.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case6.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case7.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case8.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      
      <p>Camera3: Move-Down Shot.</p>
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case9.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case10.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case11.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/dynamic_scene/dynamic_case12.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>

      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span>We demonstrate <b>dynamic results across multiple scenes</b> under three different types of camera motion. Given a reference image, camera poses, and textual descriptions, our model is able to generate the corresponding dynamic scenes.</span>
      </div>
    </div>
  </div>
</section>

<!-- Video carousel2 -->
<section class="section hero" id="More_results_2">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>10. Dynamic single scene across different camera poses.</h2>
      
      <p>Scene1: A vibrant forest scene is filled with various birds flying, surrounded by trees, green mossy ground, and sunlight.</p>
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/camera_shots_1/camera_shot1_case1.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_1/camera_shot1_case2.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_1/camera_shot1_case3.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_1/camera_shot1_case4.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
        
      <p>Scene2: A golden retriever stands in a sunlit grassy field, with trees and open green space in the background.</p>
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/camera_shots_2/camera_shot2_case1.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_2/camera_shot2_case2.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_2/camera_shot2_case3.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_2/camera_shot2_case4.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
        
      <p>Scene3: A cowboy rides a horse along a winding dirt road through a golden and sunlit field with fences.</p>
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/camera_shots_4/camera_shot4_case1.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_4/camera_shot4_case2.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_4/camera_shot4_case3.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/camera_shots_4/camera_shot4_case4.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span>We showcase the dynamic motion of the <b>same scene under different camera poses</b>, demonstrating that our model exhibits strong robustness across various scenes. The camera poses used are sourced from the MultiCamVideo dataset.</span>
      </div>
    </div>
  </div>
</section>

<!-- Video carousel2 -->
<section class="section hero is-light" id="More_results_3">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>11. Dynamic vehicle driving scene.</h2>
      
      <table class="video-table">
        <!-- Row 1 -->
        <tr>
          <td><video src="assets/more_results/driving_scene/driving_case1.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case2.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case3.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case4.mp4" loop controls muted autoplay></video></td>
        </tr>

        <!-- Row 2 -->
        <tr>
          <td><video src="assets/more_results/driving_scene/driving_case5.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case6.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case7.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case8.mp4" loop controls muted autoplay></video></td>
        </tr>

        <!-- Row 3 -->
        <tr>
          <td><video src="assets/more_results/driving_scene/driving_case9.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case10.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case11.mp4" loop controls muted autoplay></video></td>
          <td><video src="assets/more_results/driving_scene/driving_case12.mp4" loop controls muted autoplay></video></td>
        </tr>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span>Vehicle driving scenarios are among the most challenging dynamic scenes. We test our model of image-to-video generation using data from the <b>DrivingDoJo dataset</b>, and the results show that it can effectively handle common driving situations, including daytime, nighttime, straight driving, and turning.</span>
      </div>
    </div>
  </div>
</section>


<section class="section hero" id="More_results_4">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>12. Camera pose transfer: source video to generated videos.</h2>
      
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/transfer_case/transfer_case_1.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/more_results/transfer_case/transfer_case_2.mp4" loop controls muted autoplay></video></td>
        </tr>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span>Given an input video, we can obtain its camera parameters using the VGGT model, and then convert these parameters into CameraNoise with our proposed algorithm to provide the model with camera control capability. We select two scenes from the MultiCamVideo dataset and transfer the camera motion under three different reference image conditions. <b>The results demonstrate that our model can achieve lossless transfer of camera motion from the input video.</b></span>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light" id="More_results_5">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><span class="todo">[TODO #2]</span>13. Comparison with previous State-of-the-Arts.</h2>
      
      <table class="video-table">
        <tr>
          <td><video src="assets/more_results/compared_with_others/compared_case1.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/more_results/compared_with_others/compared_case3.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/more_results/compared_with_others/compared_case5.mp4" loop controls muted autoplay></video></td>
        </tr>

        <tr>
          <td><video src="assets/more_results/compared_with_others/compared_case4.mp4" loop controls muted autoplay></video></td>
        </tr>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <span class="todo">[TODO #3]</span> Since previous CameraCtrl, MotionCtrl, and Go-with-the-flow methods are trained on different datasets, we visualize their zero-shot results on the <b>MultiCamVideo dataset</b> to ensure a fair comparison. It is important to note that our focus is on controlling the camera motion in the scene, rather than dictating how the people within the scene move. GT means the ground truth.
        <p><b>We observe that all these methods exhibit varying degrees of degradation when applied to new scenes:</b></p>
        <p>1) CameraCtrl shows declines in both camera control accuracy and visual content quality;</p>
        <p>2) MotionCtrl almost completely loses its camera control capability in the new scenes;</p>
        <p>3) Go-with-the-Flow suffers from a noticeable drop in visual content quality.</p>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light" id="More_results_7">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">14. Comparison with State-of-the-Arts under OOD scenarios.</h2>
      
      <table class="video-table" style="max-width: 1300px; margin: auto; width: 100%; table-layout: fixed;">
        <tr>
          <td><video src="assets/more_results/ood_cases/ood_case2.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <p>Camera pose type 1: <span style="color: #e67e22;">move-up shot</span>.</p>

      
      <table class="video-table" style="max-width: 1300px; margin: auto; width: 100%; table-layout: fixed;">
        <tr>
          <td><video src="assets/more_results/ood_cases/ood_case3.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <p>Camera pose type 2: <span style="color: #e67e22;">move-down shot</span>.</p>

      
      <table class="video-table" style="max-width: 1300px; margin: auto; width: 100%; table-layout: fixed;">
        <tr>
          <td><video src="assets/more_results/ood_cases/ood_case4.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <p>Camera pose type 3: <span style="color: #e67e22;">move-left shot</span>.</p>
      
      
      <table class="video-table" style="max-width: 1300px; margin: auto; width: 100%; table-layout: fixed;">
        <tr>
          <td><video src="assets/more_results/ood_cases/ood_case1.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <p>Camera pose type 4: <span style="color: #e67e22;">move-right shot</span>.</p>

      
      <table class="video-table" style="max-width: 1300px; margin: auto; width: 100%; table-layout: fixed;">
        <tr>
          <td><video src="assets/more_results/ood_cases/ood_case5.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <p>Camera pose type 5: <span style="color: #e67e22;">move-clockwise shot</span>.</p>

      
      <table class="video-table" style="max-width: 1300px; margin: auto; width: 100%; table-layout: fixed;">
        <tr>
          <td><video src="assets/more_results/ood_cases/ood_case6.mp4" loop controls muted autoplay></video></td>
        </tr>
      </table>
      <p>Camera pose type 6: <span style="color: #e67e22;">move-in shot</span>.</p>

      </table>
      <div style="max-width: 1000px; margin: 0 auto;" class="has-text-justified">
        <br>
        <span>We evaluate methods MotionCtrl, CameraCtrl, Go-with-the-Flow, GEN3C, and our CameraNoise in six typical out-of-distribution (OOD) scenarios: valleys, fields, lakes, deserts, forests, and amusement parks (see the reference images in the first column). For each scene, we test these methods using six representative camera motions provided by GEN3C.</span>
        <p><b>Based on these results, we summarize the characteristics and limitations of current mainstream methods under OOD conditions:</b></p>
        <p>1) MotionCtrl and CameraCtrl: exhibit large deviations in camera following, indicating limited robustness in camera control;</p>
        <p>2) Go-with-the-Flow: prone to excessive camera motion and occasional content collapse;</p>
        <p>3) GEN3C: produces static scenes where objects cannot move, resulting in rigid video content. Additionally, due to its reliance on 3D feature modeling, it is susceptible to scene penetration issues (e.g., camera pose 5);</p>
        <p><b>4) Our method: demonstrates superior performance in OOD scenarios in terms of camera control accuracy, content consistency, and motion dynamics.</b></p>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  <script>
  </script>
</html>
